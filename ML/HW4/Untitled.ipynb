{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "punctuation=punctuation+'«»'\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тёплый запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_ready = os.path.exists('processed_data/data_processed.csv')\n",
    "data_merged = os.path.exists('processed_data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объединённые данные считаны\n"
     ]
    }
   ],
   "source": [
    "if not data_merged:\n",
    "    # файл с разметкой\n",
    "    with open('data/cluster_final_cut_train.json') as f:\n",
    "        train = json.load(f)\n",
    "\n",
    "    train_pd = pd.DataFrame.from_dict(train, orient='index').reset_index()\n",
    "    train_pd.columns = ['id', 'label']\n",
    "    train_pd['id']=train_pd['id'].astype(np.int64)\n",
    "    \n",
    "    \n",
    "    # файл с документами\n",
    "    with open('data/cosmo_content_storage_final_cut.jsonl', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    documents = [json.loads(l) for l in lines]\n",
    "\n",
    "    ids = [d['doc_id'] for d in documents]\n",
    "    urls = [d['url'] for d in documents]\n",
    "    texts = [d.get('description', None) for d in documents]\n",
    "\n",
    "    documents_pd = pd.DataFrame()\n",
    "    documents_pd['id'] = ids\n",
    "    documents_pd['url'] = urls\n",
    "    documents_pd['text'] = texts\n",
    "    \n",
    "    \n",
    "    data = pd.merge(documents_pd, train_pd, left_on='id', right_on='id', how='left')\n",
    "    data.to_csv('processed_data/data.csv')\n",
    "    print('Данные объединены')\n",
    "else:\n",
    "    data = pd.read_csv('processed_data/data.csv')\n",
    "    print('Объединённые данные считаны')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преобразование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Преобразованные данные считаны\n"
     ]
    }
   ],
   "source": [
    "if not data_is_ready:\n",
    "        #Create lemmatizer and stopwords list\n",
    "    stemmer = SnowballStemmer(\"russian\") \n",
    "    russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "    #Preprocess function\n",
    "    def preprocess_text(text):\n",
    "        def del_punct(token):\n",
    "            return ''.join([l for l in token if l not in punctuation])\n",
    "\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        #в токены\n",
    "        tokens = text.lower().split(' ')\n",
    "        #удалить пунктуацию\n",
    "        tokens = [del_punct(t) for t in tokens]\n",
    "        #удалить стоп-слова\n",
    "        tokens = [t for t in tokens if t not in punctuation]\n",
    "        #стэмминг\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        #удалить стоп-слова\n",
    "        tokens = [t for t in tokens if t not in russian_stopwords]\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    # Приведение тектс\n",
    "    data['text_arr'] = data['text'].apply(preprocess_text)\n",
    "    data['text_processed'] = data['text_arr'].apply(lambda x: ' '.join(x))\n",
    "    data.to_csv('processed_data/data_processed.csv')\n",
    "    print('Данные преобразованы')\n",
    "else:\n",
    "    data = pd.read_csv('processed_data/data_processed.csv')\n",
    "    print('Преобразованные данные считаны')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[pd.isna(data['text_processed']), 'text_processed'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сортируем по лейблу, пропуски - в конце\n",
    "data =data.sort_values(by='label').reset_index().drop(['index'], axis=1)\n",
    "\n",
    "# отюираем тренй и тест\n",
    "train = data[pd.notnull(data['label'])]\n",
    "test = data[pd.isna(data['label'])]\n",
    "\n",
    "train_size = train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель на TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(train['text_processed'], y = train['label'])\n",
    "test_matrix = vectorizer.transform(test['text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train  = lgbm.Dataset(train_matrix, label = train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_lgbm_score(params):\n",
    "    model = lgbm.train(lgbm_params, lgbm_train)\n",
    "    eval_result = model.eval(lgbm_train)\n",
    "    score = float(eval_result[eval_result.index('.')-1:])\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'objective':'multiclass', 'num_class':3064,\n",
    "              'n_estimators': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(fn=hyperopt_lgbm_score, space=simple_space_xgb, \n",
    "            algo=tpe.suggest, max_evals=10)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm_trained = lgbm.train(lgbm_params, lgbm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "submission = sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61858, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submissions/submission_{}'.format(datetime.datetime.now().strftime('%y%m%d_%H%m%S')), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
